---
title: Variational Autoencoder
subtitle: Kingma and Welling proposed an efficient algorithm for solving an intractable posterior distribution. The algorithm also works well with high dimensional and large dataset. Variational Autoencoder incorporates variational inference problem with deep learning and can be used for various distributions, but in this blog post we will focus on using the technique with Gaussian assumptions on the underlying sample distribution and that of latent variables. 
# subtitle: Praesent pulvinar risus sit amet nisl bibendum, efficitur consequat nunc accumsan. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In egestas eleifend justo, id scelerisque nisl fermentum id. In eu interdum massa. Nulla vestibulum lacus in lectus placerat, sed mollis sem mollis. Nam nec volutpat nisi, nec rutrum tortor. Quisque fringilla tortor at arcu bibendum, id eleifend sapien malesuada. Aenean non faucibus erat, nec gravida eros. Pellentesque ac egestas ipsum, eget consequat augue. Quisque molestie ornare libero in interdum.
credits: This blog post is a summary of Kingma and Welling's paper "Auto-Encoding Variational Bayes". It also borrowed ideas from Raviraja G. (2019) and Jordan J. (2018)'s blog posts on Variational Autoencoder. The links to their blog posts are provided at the end of the post. Lastly, it includes Jesse Bettencourt's CSC412 lecture material offered in Winter 2021 at University of Toronto. 
layout: default
date: 2021-03-11
moddate: 2021-03-23
readtime: 13
keywords: blogging, writing
published: true
references:  
    - Kingma, D. P., & Welling, M. (2014). Auto-Encoding Variational Bayes 
    - Jordan, J. (2018, July 16). Variational autoencoders. Jeremy Jordan. https://www.jeremyjordan.me/variational-autoencoders/. 
---
$\require{\amsbsy}$

# Variational Autoencoder 


## Usages

Stochastic variational inference is useful when the posterior is intractable and when the dataset is very large. 

Once trained, a Variational Autoencoder can serve two purposes 
1. Generator 
2. Recognition model

### Assumptions

We need the following assumptions to use stochastic variational inference proposed by Kingma and Welling.  
1. We have $N$ i.i.d. samples which are either discrete or continuous
2. These data are generated by some random process which involves unobserved random variable $\textbf{z}$. 
The relationship between $\textbf{x}$ and $\textbf{z}$ are repesented by the following graphical model. 
    <!-- $\textbf{z}^{(i)} \sim p_{\theta^*}(\textbf{z})$ and $\textbf{x}^{(i)} \sim p_{\theta}(\textbf{x}^{(i)}|\textbf{z}^{(i)})$ -->

<img height="20%" style="border: 5px solid" src="/assets/vae_directed_graph.png">

## Model 
<img height="40%" src="{{site.baseurl}}/assets/vae_diagram.png">

The encoder is a neural network where our dataset $\textbf{x}$ is passed into. Let us denote the parameters in the encoder as $\boldsymbol{\phi}$, which are the 
biases and weights in the network and denote encoder as $q_\phi(\textbf{z}|\textbf{x})$. The output of the encoder, also referred to as bottleneck, is a latent representation of our sample and is usually smaller in dimension than that of our sample. Unlike the deterministic autoencoder, variational autoencoder outputs the distribution parameters of our latent variables and the underlying distribution of the sample data from encoder and decoder respectively. 
Here we assumed that our latent variables are Gaussian distributions but it can be any distribution. We also assume that our underlying sample distributions are Gaussian distributions. 

Once the model is trained, it can be used as a recognition network or a generator model. The former requiring only the encoder and the latter only requires the decoder part of the VAE model. 

There are other variations of VAE where the $\textbf{x}$ is deterministic given $\textbf{z}$. 


## Variational Bound

Let $q_\phi(\textbf{z}\vert \textbf{x})$ be an approximate distribution of the true distribution $p(\textbf{z}\vert \textbf{x})$. Underscore $\phi$ is the parameter of the approximate posterior distribution. Our goal is to find the approximate distribution such that it is close to the true distribution. KL divergence is the metric that we use to measure the distance between the two distributions. Some of its properties will be useful later when solving for evidence lower bound. 

\[
\begin{aligned}
    \mathcal{D}_{KL}(q_\phi(\textbf{z}|\textbf{x})||p(\textbf{z}|\textbf{x}))
    &= \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})}\left[
            \log{\frac{q_\phi(\textbf{z}|\textbf{x})}{p(\textbf{z}|\textbf{x})}}
        \right]\\
    &= \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})}\left[
            \log \left(q_\phi(\textbf{z}|\textbf{x}) 
            \frac{p(\textbf{x})}{p(\textbf{x},\textbf{z})} \right)
        \right]\\
    &= \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})}\left[
        \log\frac{q_\phi(\textbf{z}|\textbf{x})}{p(\textbf{x},\textbf{z})} + \log p(\textbf{x})
        \right]\\
    &= \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})}\left[
        \log\frac{q_\phi(\textbf{z}|\textbf{x})}{p(\textbf{x},\textbf{z})}
        \right]
        + \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})}\left[
            \log p(\textbf{x})
        \right]\\
    &= \mathcal{D}_{KL}(q_\phi(\textbf{z}|\textbf{x})||p(\textbf{x},\textbf{z})) 
    + \log p(\textbf{x})\\
    &= -\mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi}; \textbf{x}^{(i)}) + \log p(\textbf{x})\\
    \\
    \log p(\textbf{x}) &= \mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi}; \textbf{x}^{(i)}) + \mathcal{D}_{KL}(q_\phi(\textbf{z}|\textbf{x})||p(\textbf{z}|\textbf{x}))
\end{aligned}
\]

We have derived the equation for marginal likelihood in terms of evidence lower bound (the first term on RHS) and the KL divergence of the approximate from the true posterior. One of the properties of KL divergence is non-negativity. Hence, we can conclude the following inequality. 

$$
    
    \begin{aligned}
        \log p(\textbf{x}) 
        &\geq \mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi}; \textbf{x}^{(i)}) \\
    \end{aligned}
$$

To maximize the likelihood of observing our samples, we maximize our evidence lower bound (ELBO). Since $\log p(\textbf{x})$ is a constant term, maximizing ELBO is equivalent to minimizing the difference between the true posterior and the approximate posterior. 


$$ 
\begin{aligned}
    \mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi}; \textbf{x})
    &= -\mathcal{D}_{KL}(q_\phi(\textbf{z}|\textbf{x})||p(\textbf{x},\textbf{z}))\\
    &= \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})}[
        - \log q_\phi(\textbf{z}|\textbf{x})
        +\log p(\textbf{x},\textbf{z})
        ]]\\
    &= \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})}\left[
        - \log q_\phi(\textbf{z}|\textbf{x})
        +\log \frac{p(\textbf{x}|\textbf{z})}{p(\textbf{z})}
        \right]\\
    &= \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})}\left[
        - \log q_\phi(\textbf{z}|\textbf{x})
        +\log p(\textbf{x}|\textbf{z})
        -\log p(\textbf{z})
        \right]\\
    &= -\mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})} \left[
        \log \frac{q_\phi(\textbf{z}|\textbf{x})}{p_\theta(\textbf{z})}
    \right]
    + \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})} [
        \log p_\theta(\textbf{x}|\textbf{z}) 
    ]\\
    &= -\mathcal{D}_{KL}(q_\phi(\textbf{z}|\textbf{x})||p_\theta(\textbf{z}))
    + \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})} [
        \log p_\theta(\textbf{x}|\textbf{z}) 
    ]
\end{aligned}
$$


The first term, equivalent to the KL divergence between the approximate posterior and the prior distribution, which measures the degree of how the approximate matches the prior and is a regularizer for $\phi$. The second term is called the reconstruction loss, which measures how well our model represents our observed sample. 

### Lower Bound Estimators

Since our lower bound is a sum of two expected values of the posterior distribution, it remains to solve the integral of the two expected values. There are efficient estimators of the lower bound. There are two estimators that we can devise using Monte Carlos estimate for expectation. 

The first estimator uses the KL divergence between the joint and posterior distribution 

$$
\begin{aligned}
    \mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi}; \textbf{x}^{(i)})
        &= \mathbb{E}_{q_\phi(\textbf{z}^{(i)}|\textbf{x}^{(i)})}[
            - \log q_\phi(\textbf{z}^{(i)}|\textbf{x}^{(i)})
            +\log p(\textbf{x}^{(i)},\textbf{z}^{(i)})
            ]]\\
    \widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta}, \boldsymbol{\phi}; \textbf{x}^{(i)})
        &\approx \frac{1}{N} 
        \sum_{l=1}^L\left[\log p(\textbf{x}^{(i)},\textbf{z}^{(i, l)}) - \log q_\phi(\textbf{z}^{(i, l)}|\textbf{x}^{(i)})
        \right]
\end{aligned}
$$

where $L$ is the dimension of the latent variable. 

The second estimator uses the lower bound in terms of reconstruction loss and KL divergence between posterior and prior. 

$$
\begin{aligned}
     \mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi}; \textbf{x}^{(i)})
        &= -\mathcal{D}_{KL}(q_\phi(\textbf{z}|\textbf{x})||p_\theta(\textbf{z}))
        + \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})} [
            \log p_\theta(\textbf{x}|\textbf{z}) 
        ]\\
    \widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta}, \boldsymbol{\phi}; \textbf{x}^{(i)})
        &\approx -\mathcal{D}_{KL}(q_\phi(\textbf{z}|\textbf{x})||p_\theta(\textbf{z}))
        + \frac{1}{L} \sum_{l=1}^L(\log p_\theta(\textbf{x}^{(i)}|\textbf{z}^{(i,l)}))

\end{aligned}
$$


## Reparametrization Trick 

We need to sample $\textbf{z} \sim q_\phi(\textbf{z}\vert \textbf{x}) = \mathcal{N}(\boldsymbol{\mu}^{(enc)}, (\boldsymbol{\sigma}^{(enc)})^2)$. But sampling directly from the is problematic because we cannot perform backpropagation through a random node. Thankfully, gaussian assumption will allow us to generate data using reparametrization trick. We will assume that $\textbf{z}$ is a deterministic variable such that $\textbf{z} = g_\phi(\boldsymbol{\epsilon}, \textbf{x})$. 
The reparametrization is 

$
\textbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \boldsymbol{\epsilon}
$

where $\boldsymbol{\epsilon}$ is a noise variable $\boldsymbol{\epsilon} \sim \mathcal{N}(\textbf{0}, \textbf{I})$. 

There are different ways to choose $p(\boldsymbol{\epsilon})$ such as uniform distribution or in some cases we may even use different transformation $\boldsymbol{\epsilon}$ depending on what our assumptive underlying distribution is. For the purposes of approximating normal distribution, the above reparametrization is trick is the most appropriate. 

The left directed graph in the image below samples $\textbf{z}$ from a normal distribution, which means we cannot get its gradient with respect to the distribution parameters due to the randomness. Using the reparametrization trick, we have $\textbf{z}$ in terms of a function of $\boldsymbol{\epsilon}$ and $\textbf{x}$. 

<img src="{{site.baseurl}}/assets/reparametrization.png" width="70%">
<p style="text-align:center;font-size:10px">Image credit: Jeremy Jordan, 2018</p>


With reparametrization trick, we can now optimize the parameters. 

<img src="{{site.baseurl}}/assets/reparametrization_backprop.png" width="70%">
<p style="text-align:center;font-size:10px">Image credit: Jeremy Jordan, 2018</p>



## ELBO Loss Function, Gaussian case


Recall that the ELBO lower bound consists of log likelihood of observing our data and the KL divergence between the posterior and the prior distribution. Our objective is to minimize the KL divergence term and maximize the log likelihood, which is same as maximizing ELBO, or minimizing the negative of ELBO. 

$$
\begin{aligned}
loss &= - \mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi}; \textbf{x}^{(i)})\\
 &= \mathcal{D}_{KL}(q_\phi(\textbf{z}|\textbf{x})||p_\theta(\textbf{z}))
    - \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})} [
        \log p_\theta(\textbf{x}|\textbf{z}) 
    ]
\end{aligned}
$$

The second reconstruction error term can be replaced with any error function that compared the original data and the generated data such as mean squared error function if the VAE only estimated posterior distribution. But in the following, we will assume that the VAE estimated both the posterior and the underlying sample distribution by encoder and decoder respectively. 

There are two ways to write the loss function in Gaussian case. 

<!-- $$
\newcommand{\expec}{\mathbb{E}}
\begin{aligned}
    \widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta}, \boldsymbol{\phi}; \textbf{x}^{(i)})
        &= \expec_{q_\phi(\textbf{z}|\textbf{x})}[\log(q_\phi(\textbf{z}|\textbf{x})) - \log(p(\textbf{x},\textbf{z}))]\\
        &\approx \sum_n \left[\log(q_\phi(z_n|x_n)) - \log(p(x_n,z_n))\right]\\
        % TODO: Explain that this is Monte Carlos estimation 
        &= \sum_n \left[\log(q_\phi(z_n|x_n)) - \log(p(x_n|z_n)) - \log(p(z_n))\right]\\
        &= \sum_n \left[\sum_l \log(q_\phi(z_{nl}|x_n)) - \sum_{l} \log(p(z_{nl}))\right] - \sum_n \sum_{d} \log(p(x_{nd}|z_n))\\
\end{aligned}
$$ -->


#### Method 1: Using Simple Monte Carlos Estimator


$$
\begin{aligned}
loss &= \mathcal{D}_{KL}(q_\phi(\textbf{Z}|\textbf{X})||p_\theta(\textbf{Z}))
    - \mathbb{E}_{q_\phi(\textbf{Z}|\textbf{X})} [
        \log p_\theta(\textbf{X}|\textbf{Z}) 
    ]\\
    &= \mathbb{E}_{q_\phi(\textbf{Z}|\textbf{X})}[\log(q_\phi(\textbf{Z}|\textbf{X})) - \log(p_\theta(\textbf{Z}))]
    - \mathbb{E}_{q_\phi(\textbf{Z}|\textbf{X})} [
        \log p_\theta(\textbf{X}|\textbf{Z}) 
    ]\\
    &\approx \sum_n \left[\log(q_\phi(\textbf{z}_{n}|\textbf{x}_n)) - \log(p(\textbf{z}_{n}))\right] - \sum_n \log(p(\textbf{x}_{n}|\textbf{z}_n))\\
    &= \sum_n \left[\sum_l \log(q_\phi(z_{nl}|x_n)) - \sum_{l} \log(p(z_{nl}))\right] - \sum_n \sum_{d} \log(p(x_{nd}|z_n))\\
\end{aligned}
$$

where $n$ is a single data, $l$ is the latent dimension, and $d$ is the dimension of input data.

Here we have converted KL divergence into its expected value equivalence and used 
simple Monte Carlo estimates of expectation on all expected values. 

Given the assumption that the underlying distributions are Gaussian, we can quantitatively solve the above equation with probabilities. 

The following is the log likelihood of a normal distribution, which we will plug into the equation

$$
\log \mathcal{N}(x|\mu, \sigma^2) = -\frac{1}{2} \log(2 \pi) - \log(\sigma) - \frac{(x-\mu)^2}{2\sigma^2}
$$

Finally, we get the following equation for minimum KL 

$$
\begin{aligned}
    \sum_n &\sum_l \log(q_\phi(z_{nl}|x_n)) - \sum_n\sum_{l} \log(p_\theta(z_{nl})) - \sum_n \sum_{d} \log(p_\theta(x_{nd}|z_n))\\
    &= \sum_n \sum_l \left(
                 -\frac{1}{2} \log(2 \pi) 
                 - \log((\sigma_{nl}^{(enc)})^2) 
                 - \frac{(x_{nl} - \mu_{nl}^{(enc)})^2}{2(\sigma_{nl}^{(enc)})^2}
            \right)\\
    &+ \sum_n \sum_d \left(
        \frac{1}{2} \log(2\pi)
        + \log((\sigma_{nd}^{(dec)})^2)
        + \frac{(x_{nd} - \mu_{nd}^{(dec)})^2}{2(\sigma_{nd}^{(dec)})^2}
        \right)\\
    &+ \sum_n \sum_l \left(
        \frac{1}{2} \log(2 \pi)
        + \frac{(z_{nl})^2}{2}
    \right)
\end{aligned}
$$

<!-- We have the equation that measures how close the approximate posterior is to the true posterior. Since the objective of our variational inference is to find some posterior which best estimates the true posterior, the objective of our variational inference is to minimize the above equation. In other words, find $\phi$ such that the KL divergence is minimized. 

$$
\begin{aligned}
    \argmin_\phi \mathcal{D}_{KL}(q_\phi(\textbf{z}|\textbf{x})||p(\textbf{z}|\textbf{x})) 
\end{aligned}
$$ -->

In this case, we can remove all the constant terms because they do not influence the direction of the gradient. 

$$
\begin{aligned}
    loss = \sum_n &\sum_l \left(
        - \log((\sigma_{nl}^{(enc)})^2) 
        - \frac{(x_{nl} - \mu_{nl}^{(enc)})^2}{2(\sigma_{nl}^{(enc)})^2}
    \right)\\
    &+ \sum_n \sum_d \left(
        \log((\sigma_{nd}^{(dec)})^2)
        + \frac{(x_{nd} - \mu_{nd}^{(dec)})^2}{2(\sigma_{nd}^{(dec)})^2}
    \right) \\
    &+ \sum_n \sum_l \left(
        \frac{(z_{nl})^2}{2}
    \right)

\end{aligned}
$$

#### Method 2: Solving KL divergence, Gaussian case

ELBO in KL-divergence term can be integrated analytically. The following is derivation of the solution when the underlying distributions are Gaussian. 

$$
\begin{aligned}
    \mathcal{D}_{KL}(q_\phi(\textbf{z}|\textbf{x})||p_\theta(\textbf{z}))
    &= \int q_\phi(\textbf{z}|\textbf{x}) \log q_\phi(\textbf{z}|\textbf{x}) d \textbf{z} -  \int q_\phi(\textbf{z}|\textbf{x}) \log p_\theta(\textbf{z}) d \textbf{z}\\
    &= \int \mathcal{N}(\textbf{z};\boldsymbol{\mu}_\phi, \boldsymbol{\sigma}_\phi^2) \log \mathcal{N}(\textbf{z};\boldsymbol{\mu}_\phi, \boldsymbol{\sigma}_\phi^2) d\textbf{z} - \int \mathcal{N}(\textbf{z};\boldsymbol{\mu}_\phi, \boldsymbol{\sigma}_\phi^2) \log \mathcal{N}(\textbf{z};\textbf{0}, \textbf{I}) d\textbf{z} \\
    &= \left[
            -\frac{L}{2} \log(2\pi) - \frac{1}{2}\sum_{l=1}^L (1 + \log \sigma_l^2)
        \right]
        - \left[
            -\frac{L}{2}\log(2\pi) - \frac{1}{2} \sum_{l=1}^L (\mu_l^2 + \sigma_l^2)
            \right]\\
    &= - \frac{1}{2}\sum_l (1 + \log \sigma_l^2) + \frac{1}{2} \sum_{l=1}^L (\mu_l^2 + \sigma_l^2)\\
    &= - \frac{1}{2} \sum_{l} \left(1 + \log(\sigma_l^2) - \mu_l^2 - \sigma_l^2\right)
\end{aligned}
$$

We can combine the KL divergence term with the Monte Carlos estimate 

$$
\begin{aligned}
    loss &= \mathcal{D}_{KL}(q_\phi(\textbf{Z}|\textbf{X})||p_\theta(\textbf{Z}))
            - \mathbb{E}_{q_\phi(\textbf{Z}|\textbf{X})} [
            \log p_\theta(\textbf{X}|\textbf{Z}) 
        ]\\
    &= - \frac{1}{2}  \sum_n \sum_{l} \left(1 + \log((\sigma_l^{(i)})^2) - (\mu_l^{(i)})^2 - (\sigma_l^{(i)})^2\right) 
        - \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})} [
            \log p_\theta(\textbf{x}|\textbf{z}) 
        ]\\
    &\approx - \frac{1}{2} \sum_n \sum_{l} \left(1 + \log((\sigma_l^{(i)})^2) - (\mu_l^{(i)})^2 - (\sigma_l^{(i)})^2\right) 
        - \sum_n \sum_{d} \log(p(x_{nd}|z_n))
\end{aligned}
$$

